# -*- coding: utf-8 -*-
"""twitter sentiment analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U65aYuYMfMXWuB2WP6mMHBVJRfxiaoyQ
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import re
import nltk

data = pd.read_csv("https://raw.githubusercontent.com/amankharwal/Website-data/master/twitter.csv")
print(data.head())

nltk.download('stopwords')
stemmer = nltk.SnowballStemmer("english")
from nltk.corpus import stopwords
import string
stopword=set(stopwords.words('english'))

"""remove special characters"""

data['clean_tweet'] = data['tweet'].replace(regex='(@\w+)|#|&|!',value='')

data.head()

data

"""remove digits from tweets"""

data['clean_tweet'] = data['clean_tweet'].str.replace('\d+', '')

data.head()

data

"""convert to lower case"""

data['clean_tweet'] = data['clean_tweet'].apply(str.lower)

data

#s=data.set_index('clean_tweet').Col2.str.get_dummies(sep=' ')
#s.loc[:,s.all()].stack().reset_index(level=1).groupby('Col1')['level_1'].apply(' '.join).reset_index(name='Col2')

"""Handle contractions (e.g., "can't" to "cannot") to avoid """

pip install contractions

import contractions

#data['clean_tweet'] = data['clean_tweet'].apply(lambda x: [contractions.fix(word) for word in x.split()])
#data.head()

import nltk
nltk.download('punkt')  # Download the required NLTK data

def add_not_prefix(text):
    tokens = nltk.word_tokenize(text)  # Tokenize the text into individual words
    modified_tokens = []
    is_negation = False

    for token in tokens:
        if token.lower() in ['not', 'no', 'never']:
            is_negation = True
        elif token.isalpha() and is_negation:
            token = 'NOT_' + token
            is_negation = False

        modified_tokens.append(token)

    return ' '.join(modified_tokens)

# Apply the function to the 'Text' column
data['tokenized'] = data['clean_tweet'].apply(add_not_prefix)
data.head()

"""Tokenize"""

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer

data['tokenized'] = data['clean_tweet'].apply(word_tokenize)
data

"""punctuation removal"""

punc = string.punctuation
data['no_punc'] = data['tokenized'].apply(lambda x: [word for word in x if word not in punc])
data.head()

"""stop word removal"""

stop_words = set(stopwords.words('english'))
data['stopwords_removed'] = data['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])
data.head()

import nltk
nltk.download('averaged_perceptron_tagger')

"""stemming & lemmetization"""

data['pos_tags'] = data['stopwords_removed'].apply(nltk.tag.pos_tag)
data.head()

import nltk
nltk.download('wordnet')

def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
data['pos_tags'] = data['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])
data.head()

wnl = WordNetLemmatizer()
data['lemmatized'] = data['pos_tags'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])
data.head()

data['lemmatized_string'] = data['lemmatized'].apply(lambda x: ' '.join(x))

#import pandas as pd
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')
#data = pd.read_csv('your_dataset.csv')
sia = SentimentIntensityAnalyzer()
sentiment_scores = []

for tweet in data['tweet']:
    scores = sia.polarity_scores(tweet)
    
    if scores['compound'] > 0:
        sentiment = 'happy'
    elif scores['compound'] < 0:
        sentiment = 'sad'
    else:
        sentiment = 'neutral'
    
    sentiment_scores.append(sentiment)
data['sentiment'] = sentiment_scores
#data.to_csv('sentiment_scores.csv', index=False)

data

unique_values = data['sentiment'].value_counts()
#sum_by_unique_values = unique_values.sum()
highest_count_value = unique_values.idxmax()
print(unique_values)
print("Most of the tweets are", highest_count_value)

